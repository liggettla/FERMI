#!/usr/bin/env python

# The purpose of this script is to extract variant numbers from all positions
# along probed regions to quantify the probabilities that sites containing
# only single mutations could have been the result of chance.

############
# Argparse #
############
def runArgparse():
    import argparse
    from numpy import mean

    parser = argparse.ArgumentParser()
    parser.add_argument('--inDir', '-i', type=str, help='Specifies the input directory containing analsis directories.')
    #parser.add_argument('--wt', '-w', type=str, help='Specify the WT base that is getting mutated.')
    #parser.add_argument('--mut', '-m', type=str, help='Specify the mutant base.')

    args = parser.parse_args()
    inDir = args.inDir
    #wt = args.wt
    #mut = args.mut

    return inDir


# this builds a file of commands to download UCSC DNA segments
# surrounding base change of interest
def outputLocs(variants, wt, mut):
    target = open('commands.sh', 'w')
    # erase previous work
    locs = open('locs', 'w')
    locs.close()
    target.write('#!/usr/bin/env bash\n')

    for loc in variants:
        if variants[loc]['wt'] == wt and variants[loc]['var'] == mut: 
            low = int(loc) - 10
            high = int(loc) + 10
            target.write('wget -O - http://genome.ucsc.edu/cgi-bin/das/hg19/dna?segment=%s:%s,%s >> locs\n' % (variants[loc]['chrom'], low, high))

    target.close()
    import sys, os
    os.system('./commands.sh')

# output all of the sequences surrounding
# base of interest
def cleanLocs():
    target = open('locs', 'r')
    output = open('sequences.txt', 'w')

    for line in target:
        if '<' not in line and 'n' not in line:
            output.write(line)
        else:
            pass

    target.close()
    output.close()

def quantifyFlanks():

    #{1:{'a':5,'t':6,'g':1,'c':3}}
    #{'locus':[1,1,1,1],'percent':[0.25,0.25,0.25,0.25],'letters':['t','c','g','a']
    bases = {}
    target = open('sequences.txt', 'r')
    for line in target:
        line = line.rstrip('\n')
        count = 1
        for base in line:
            if count in bases:
                bases[count][base]+=1
            else:
                bases[count]={'t':0,'c':0,'g':0,'a':0}
                bases[count][base]+=1
            count += 1

    return bases


def plotSeq(bases): 
    import matplotlib.pyplot as plt
    import pandas as pd

    d = {1: {'a': 140, 'c': 173, 't': 128, 'g': 136}, 2: {'a': 145, 'c': 161, 't': 138, 'g': 133}}

    df = pd.DataFrame.from_dict(bases, orient='index').apply(lambda p: p/p.sum(), axis=1)

    ax = df.plot(kind='bar', rot=0, ylim=(0, 0.4))
    ax.set_xlabel('Position')
    ax.set_ylabel('Percent')

    plt.show()
    


def buildDF(inFile, badLocs, goodLocs):
    from parseline import parseLine
    import pandas as pd
    print inFile
    target = open(inFile, 'r')
    #variants = {loc:'CT'}
    variants = {}

    for line in target:
        location, AFNum, WT, var, loc, chrom = parseLine(line)
        if loc: # parseLine returns false if within info lines

            # flag anything that is mutated differently within this vcf
            if loc in variants and loc not in badLocs:
                badLocs.append(loc)

            # if location prev ID'd as good check that change was the same
            if loc in goodLocs:
                if goodLocs[loc] == '%s-%s' % (WT, var):
                    variants[loc] = '%s-%s' % (WT, var)
                else:
                    badLocs.append(loc)

            # if never seen, assume good and add
            else:
                goodLocs[loc] = '%s-%s' % (WT, var)
                variants[loc] = '%s-%s' % (WT, var)

    # delete any variants that aren't the only variants found
    # at a given locus but will miss those that haven't been found to be bad
    # these must be removed later
    for i in badLocs:
        if i in variants:
            del variants[i]


    # create pandas dataframe from the dictionary
    # the index orient will use the locs as indices
    df = pd.DataFrame.from_dict(variants, orient='index')
    # transform so locations are column headers
    df = df.T

    return df, badLocs

def combineDFs(allData,df):
    # this comines the data from newly parsed vcf files
    # together with total d
    # ignore index will give each sample a new number
    allData=allData.append(df, ignore_index=True)
    return allData

def getFiles(inDir):
    # builds a list of all files to investigate from
    # the input directory
    from glob import glob as g
    fileList = g('%s/*fastq/onlyProbedRegions.vcf' % (inDir))
    return fileList

def removeBadLocs(allData, badLocs):
    # because badLocs are sometimes found only after the first
    # few files are parsed, locs with multiple variants must be
    # removed after building dataframe
    for loc in badLocs:
        if loc in allData:
            # 0 drops rows, 1 drops columns
            allData = allData.drop(loc,1)

    return allData


    

    


if __name__ == '__main__':
    import pandas as pd
    inDir = runArgparse()
    fileList = getFiles(inDir)
    allData = pd.DataFrame()
    badLocs = [] # keep track of locs with multiple vars
    goodLocs = {} # keep track of variant type at each loc
    # parse all input files
    for inputFile in fileList:
        df, badLocs = buildDF(inputFile, badLocs, goodLocs)
        allData = combineDFs(allData, df)
    # remove any missed badlocs
    allData = removeBadLocs(allData, badLocs)

    print allData




