#!/usr/bin/env python

# The purpose of this script is to extract variant numbers from all positions
# along probed regions to quantify the probabilities that sites containing
# only single mutations could have been the result of chance.

############
# Argparse #
############
def runArgparse():
    import argparse
    from numpy import mean

    parser = argparse.ArgumentParser()
    parser.add_argument('--inDir', '-i', type=str, help='Specifies the input directory containing analsis directories.')

    args = parser.parse_args()
    inDir = args.inDir

    return inDir

def buildDF(inFile, badLocs, goodLocs, totalVarCounts):
    from parseline import parseLine
    import pandas as pd
    target = open(inFile, 'r')
    #variants = {loc:'CT'}
    variants = {}

    for line in target:
        location, AFNum, WT, var, loc, chrom = parseLine(line)
        if loc: # parseLine returns false if within info lines
            # only look at substitutions
            if len('%s-%s' % (WT, var)) == 3:
                totalVarCounts['%s-%s' % (WT, var)] += 1

            # flag anything that is mutated differently within this vcf
            if loc in variants and loc not in badLocs:
                badLocs.append(loc)

            # if location prev ID'd as good check that change was the same
            if loc in goodLocs:
                if goodLocs[loc] == '%s-%s' % (WT, var):
                    variants[loc] = '%s-%s' % (WT, var)
                else:
                    badLocs.append(loc)

            # if never seen, assume good and add
            else:
                goodLocs[loc] = '%s-%s' % (WT, var)
                variants[loc] = '%s-%s' % (WT, var)

    # delete any variants that aren't the only variants found
    # at a given locus but will miss those that haven't been found to be bad
    # these must be removed later
    for i in badLocs:
        if i in variants:
            del variants[i]


    # create pandas dataframe from the dictionary
    # the index orient will use the locs as indices
    df = pd.DataFrame.from_dict(variants, orient='index')
    # transform so locations are column headers
    df = df.T

    return df, badLocs, totalVarCounts

def combineDFs(allData,df):
    # this comines the data from newly parsed vcf files
    # together with total d
    # ignore index will give each sample a new number
    allData=allData.append(df, ignore_index=True)
    return allData

def getFiles(inDir):
    # builds a list of all files to investigate from
    # the input directory
    from glob import glob as g
    fileList = g('%s/*fastq/onlyProbedRegions.vcf' % (inDir))
    return fileList

def removeBadLocs(allData, badLocs):
    # because badLocs are sometimes found only after the first
    # few files are parsed, locs with multiple variants must be
    # removed after building dataframe
    for loc in badLocs:
        if loc in allData:
            # 0 drops rows, 1 drops columns
            allData = allData.drop(loc,1)

    return allData

def countVariants(allData):
    # this counts each type of variant
    from collections import defaultdict
    counts = defaultdict(int)

def mutationRatios(totalVarCounts):
    # this computes the ratios of each substitution changes
    total = 0
    for i in totalVarCounts:
        total += totalVarCounts[i]


    for i in totalVarCounts:
        print('%s:%s' % (i,str(float(totalVarCounts[i])/float(total))))

def defineProbabilities():
    #x = {'T-A':0.2886282509,'T-G':0.008669046405,'T-C':0.7027027027,'C-A':0.1144487215,'C-G':0.005361561727,'C-T':0.8801897168,'G-C':0.1411374931,'G-A':0.3717283269,'G-T':0.48713418,'A-G':0.6341116599,'A-C':0.006061966771,'A-T':0.3598263733}
    x = {'T-A':0.1045736252,'T-G':0.004353965639,'T-C':0.2457048717,'T-T':0.6453675375,'C-A':0.07028799544,'C-G':0.003813800969,'C-T':0.506201882,'C-C':0.4196963216,'G-C':0.05708131395,'G-A':0.1481652435,'G-T':0.1937841372,'G-G':0.6009693053,'A-G':0.3658376711,'A-C':0.004704067862,'A-T':0.2100250627,'A-A':0.4194331984}

    return x

def calculateProbs(allData, ratios):
    from collections import defaultdict
    from scipy.misc import comb
    allProbs = defaultdict(list)

    for loc in allData:
        count = 0
        varType = ''
        for variant in allData[loc]:
            # count variant instances
            if pd.notnull(variant) and len(variant) == 3:
                varType = variant
                count += 1
        if count > 0:
            # simple probability
            #probability = pow(ratios[varType],count*2)

            # binomial probability
            probability = comb(21,count)*pow(ratios[varType],count)*pow((1-ratios[varType]),(21-count))
            allProbs[varType].append(probability)

    # now sort all of the lists in descending order
    for i in allProbs:
        list.sort(allProbs[i],reverse=True)
    
    # correct for multicomparisons
    allProbs = multicompcorrect(allProbs)

    # calculate p values
    fisherpvalues(allProbs)

    return allProbs

def multicompcorrect(allProbs):
    # bonferroni correct for multiple comparisons
    import statsmodels.sandbox.stats.multicomp as mc
    for i in allProbs:
        x = mc.multipletests(allProbs[i])
        allProbs[i] = list(x[1])

    return allProbs

def fisherpvalues(allProbs):
    # calculate p value by fisher's test
    # will return a dictionary with p values for each variant
    from scipy.stats import combine_pvalues as fisher

    for i in allProbs:
        stats = fisher(allProbs[i])
        chi = stats[0]
        p_val = stats[1]
        print i, p_val




def plotProbabilities(allProbs):
    import matplotlib.pyplot as plt

    title_font = {'fontname':'Arial', 'size':'30', 'color':'black', 'weight':'normal'}
    axis_font = {'fontname':'Arial', 'size':'30'}

    # plot as p values
    for i in allProbs:
        plt.bar(range(len(allProbs[i])),allProbs[i],color='gray')
        plt.title('%s' % (i), **title_font)
        plt.ylabel('p-value', **axis_font)
        plt.xlabel('Individual Variants', **axis_font)
        plt.yscale('log')
        plt.xticks(**axis_font)
        plt.yticks(**axis_font)
        plt.tight_layout()

        plt.savefig('plot.png', figsize=(13,13), dpi=1750)
        plt.show()

def qqplot(allProbs, plottype='both'):
    # the purpose of this is to generate a qq plot from the data
    import numpy as np
    import pylab
    import scipy.stats as stats

    if plottype == 'probability' or plottype == 'both':
        for variant in allProbs:
            stats.probplot(allProbs[variant], dist="norm", plot=pylab)
            pylab.title('Probability Plot %s' % (variant), fontsize=40)
            pylab.ylabel('Ordered Values', fontsize=40)
            pylab.xlabel('Theoretical Quantiles', fontsize=40)
            pylab.yticks(fontsize=40)
            pylab.xticks(fontsize=40)
            pylab.tight_layout()
            pylab.show()

    if plottype == 'qq' or plottype == 'both':
        for variant in allProbs:
            import statsmodels.api as sm
            from matplotlib import pyplot as plt
            fig = sm.qqplot(np.asarray(allProbs[variant]), stats.t, fit=True, line='45')
            plt.title('Q-Q Plot %s' % (variant), fontsize=40)
            plt.ylabel('Ordered Values', fontsize=40)
            plt.xlabel('Theoretical Quantiles', fontsize=40)
            plt.yticks(fontsize=40)
            plt.xticks(fontsize=40)
            plt.tight_layout()
            plt.show()

def buildInclusiveDF(inputFile, totalVarCounts):
    # this method parses the vcf file passed to it and builds a pd df 
    # with variants sorted by locations. Unlike buildDF(), this method is
    # independent of a variant's uniqueness, and all found variants will be
    # included in the df.
    target = open(inputFile, 'r')

    #variants = {loc:['C-T','C-A']}
    variants = defaultdict(list)

    for line in target:
        location, AFNum, WT, var, loc, chrom = parseLine(line)
        if loc: # parseLine returns false if within info lines
            # only look at substitutions
            if len('%s-%s' % (WT, var)) == 3:
                totalVarCounts['%s-%s' % (WT, var)] += 1
                variants[loc].append('%s-%s' % (WT, var))

    # create pandas dataframe from the dictionary
    # the index orient will use the locs as indices
    # and transform so locations are column headers
    df = pd.DataFrame.from_dict(variants, orient='index').T

    return df, totalVarCounts

# this will add expected counts of no mutation at particular sites
# these are unique to probed region and must be changed to fit probes
# being used
def varProbability(totalVarCounts, fileCount):
    #totalVarCounts = {'A-C': 1500, 'G-C': 1000}
    baseCounts = {'T':fileCount*1214, 'C':fileCount*1336, 'G':fileCount*1238, 'A':fileCount*1235}
    # observedCounts = {'T':5000, 'C':4000}
    observedCounts = defaultdict(int)
    # get total numbers of variants per wt base
    for variant in totalVarCounts:
        numVars = totalVarCounts[variant]
        wtBase = variant[0]
        observedCounts[wtBase] += numVars

    variantProbs = {}
    # get percentage representations multiplied by filecount
    for variant in totalVarCounts:
        variantProbs[variant] = float(totalVarCounts[variant]) / float(observedCounts[variant[0]])

    return variantProbs 

# this will return a df of column counts
def countAllVars(allData):
    for loc in allData:
        countsdf = allData[loc].value_counts()

    return countsdf

def calculateProbabilities(allData, variantProbs):
    allProbs = defaultdict(list)
    # this will get lists of variants and their counts at each location
    for loc in allData:
        tempVarProbs = {}
        # [18, 15, 1]
        counts = allData[loc].value_counts().tolist()
        numVars = sum(counts)
        # ['C-T', 'C-A', 'C-G']
        variants = allData[loc].value_counts().index.tolist()

        # calc expected numbers of each variant based on number of variants
        # seen at this location
        # {'G-C': 0.25}
        for i in variantProbs:
            tempVarProbs[i] = variantProbs[i] * numVars 
        
        stat, p, wtbase = getChiSquared(counts, variants, tempVarProbs)
        
        allProbs[wtbase].append(p)

    # now sort all of the lists in descending order
    for i in allProbs:
        list.sort(allProbs[i],reverse=True)

    # correct for multicomparisons
    allProbs = multicompcorrect(allProbs)

    # calculate total p values
    fisherpvalues(allProbs)

    return allProbs

def getChiSquared(counts, variants, tempVarProbs):
    # this puts two lists together of observed and expected numbers of variants
    # at given location, compares them, and returns chi-squared and p values
    possibleVars = {'T':['T-C','T-G','T-A'],'C':['C-T','C-G','C-A'],'G':['G-T','G-C','G-A'],'A':['A-T','A-C','A-G']}
    observed = []
    expected = []
    varList = []

    wtbase = ''
    # start by adding all observed variants and expected frequencies
    for x, y in zip(variants, counts):
        wtbase = x[0]
        observed.append(y)
        expected.append(tempVarProbs[x])
        varList.append(x)
    # add zeros for variants that are not observed
    for i in possibleVars[wtbase]:
        if i not in varList:
            # [17, 16, 2]
            observed.append(0)
            # [15.13598369011213, 17.594903160040776, 2.2691131498470947]
            expected.append(tempVarProbs[i])
            # ['A-T', 'A-G', 'A-C']
            varList.append(i)

    # calculate chi squared
    x = chi(observed, f_exp=expected)
    stat = x[0]
    p = x[1]

    return stat, p, wtbase

def originalMethod():
    # this is the original method we were using to calculate significance
    # it appears that this was incorrect for a number of reasons, but for
    inDir = runArgparse()
    fileList = getFiles(inDir)
    allData = pd.DataFrame()
    badLocs = [] # keep track of locs with multiple vars in single vcf
    goodLocs = {} # keep track of variant type at each loc
    totalVarCounts = defaultdict(int)
    # parse all input files
    for inputFile in fileList:
        df, badLocs, totalVarCounts = buildDF(inputFile, badLocs, goodLocs, totalVarCounts)
        allData = combineDFs(allData, df)
    # remove any missed badlocs
    allData = removeBadLocs(allData, badLocs)
    countVariants(allData)
    #mutationRatios(totalVarCounts)
    ratios = defineProbabilities()
    # calculate all the probabilites for every locus
    allProbs = calculateProbs(allData, ratios)
    qqplot(allProbs)
    plotProbabilities(allProbs)
    # reproducibility's sake this is being kept available

def newMethod():
    # this method now uses chi squared instead of binomial probability
    # and does not rely on variants being uniquely observed at a particular site
    inDir = runArgparse()
    fileList = getFiles(inDir)
    
    '''
       229041219 229041220 229041221 229041222 229041223 229041224 229041225  
    0        A-G       NaN       NaN       NaN       G-T       C-T       A-G 
    '''
    allData = pd.DataFrame() # structure shown above
    totalVarCounts = defaultdict(int) # keep track of variant numbers to get expected counts

    # parse input files and put in pd df with loc as columns and index as rows
    fileCount = 0
    for inputFile in fileList:
        fileCount += 1
        print ('Parsing: %s' % (inputFile))
        df, totalVarCounts = buildInclusiveDF(inputFile, totalVarCounts)
        allData = combineDFs(allData, df)
    allCounts = countAllVars(allData)
    # variantProbs = {'C-T':0.58}
    variantProbs = varProbability(totalVarCounts, fileCount)
    allProbs = calculateProbabilities(allData, variantProbs)
    # qq and probability plots
    qqplot(allProbs)
    # barplot
    plotProbabilities(allProbs)

if __name__ == '__main__':
    import pandas as pd
    from collections import defaultdict
    from parseline import parseLine
    from scipy.stats import chisquare as chi

    #originalMethod()
    newMethod()





