#!/usr/bin/env python

# The purpose of this script is to extract variant numbers from all positions
# along probed regions to quantify the probabilities that sites containing
# only single mutations could have been the result of chance.

############
# Argparse #
############
def runArgparse():
    import argparse
    from numpy import mean

    parser = argparse.ArgumentParser()
    parser.add_argument('--inDir', '-i', type=str, help='Specifies the input directory containing analsis directories.')
    #parser.add_argument('--wt', '-w', type=str, help='Specify the WT base that is getting mutated.')
    #parser.add_argument('--mut', '-m', type=str, help='Specify the mutant base.')

    args = parser.parse_args()
    inDir = args.inDir
    #wt = args.wt
    #mut = args.mut

    return inDir


# this builds a file of commands to download UCSC DNA segments
# surrounding base change of interest
def outputLocs(variants, wt, mut):
    target = open('commands.sh', 'w')
    # erase previous work
    locs = open('locs', 'w')
    locs.close()
    target.write('#!/usr/bin/env bash\n')

    for loc in variants:
        if variants[loc]['wt'] == wt and variants[loc]['var'] == mut: 
            low = int(loc) - 10
            high = int(loc) + 10
            target.write('wget -O - http://genome.ucsc.edu/cgi-bin/das/hg19/dna?segment=%s:%s,%s >> locs\n' % (variants[loc]['chrom'], low, high))

    target.close()
    import sys, os
    os.system('./commands.sh')

# output all of the sequences surrounding
# base of interest
def cleanLocs():
    target = open('locs', 'r')
    output = open('sequences.txt', 'w')

    for line in target:
        if '<' not in line and 'n' not in line:
            output.write(line)
        else:
            pass

    target.close()
    output.close()

def quantifyFlanks():

    #{1:{'a':5,'t':6,'g':1,'c':3}}
    #{'locus':[1,1,1,1],'percent':[0.25,0.25,0.25,0.25],'letters':['t','c','g','a']
    bases = {}
    target = open('sequences.txt', 'r')
    for line in target:
        line = line.rstrip('\n')
        count = 1
        for base in line:
            if count in bases:
                bases[count][base]+=1
            else:
                bases[count]={'t':0,'c':0,'g':0,'a':0}
                bases[count][base]+=1
            count += 1

    return bases


def plotSeq(bases): 
    import matplotlib.pyplot as plt
    import pandas as pd

    d = {1: {'a': 140, 'c': 173, 't': 128, 'g': 136}, 2: {'a': 145, 'c': 161, 't': 138, 'g': 133}}

    df = pd.DataFrame.from_dict(bases, orient='index').apply(lambda p: p/p.sum(), axis=1)

    ax = df.plot(kind='bar', rot=0, ylim=(0, 0.4))
    ax.set_xlabel('Position')
    ax.set_ylabel('Percent')

    plt.show()
    


def buildDF(inFile, badLocs, goodLocs, totalVarCounts):
    from parseline import parseLine
    import pandas as pd
    print inFile
    target = open(inFile, 'r')
    #variants = {loc:'CT'}
    variants = {}

    for line in target:
        location, AFNum, WT, var, loc, chrom = parseLine(line)
        if loc: # parseLine returns false if within info lines
            # only look at substitutions
            if len('%s-%s' % (WT, var)) == 3:
                totalVarCounts['%s-%s' % (WT, var)] += 1

            # flag anything that is mutated differently within this vcf
            if loc in variants and loc not in badLocs:
                badLocs.append(loc)

            # if location prev ID'd as good check that change was the same
            if loc in goodLocs:
                if goodLocs[loc] == '%s-%s' % (WT, var):
                    variants[loc] = '%s-%s' % (WT, var)
                else:
                    badLocs.append(loc)

            # if never seen, assume good and add
            else:
                goodLocs[loc] = '%s-%s' % (WT, var)
                variants[loc] = '%s-%s' % (WT, var)

    # delete any variants that aren't the only variants found
    # at a given locus but will miss those that haven't been found to be bad
    # these must be removed later
    for i in badLocs:
        if i in variants:
            del variants[i]


    # create pandas dataframe from the dictionary
    # the index orient will use the locs as indices
    df = pd.DataFrame.from_dict(variants, orient='index')
    # transform so locations are column headers
    df = df.T

    return df, badLocs, totalVarCounts

def combineDFs(allData,df):
    # this comines the data from newly parsed vcf files
    # together with total d
    # ignore index will give each sample a new number
    allData=allData.append(df, ignore_index=True)
    return allData

def getFiles(inDir):
    # builds a list of all files to investigate from
    # the input directory
    from glob import glob as g
    fileList = g('%s/*fastq/onlyProbedRegions.vcf' % (inDir))
    return fileList

def removeBadLocs(allData, badLocs):
    # because badLocs are sometimes found only after the first
    # few files are parsed, locs with multiple variants must be
    # removed after building dataframe
    for loc in badLocs:
        if loc in allData:
            # 0 drops rows, 1 drops columns
            allData = allData.drop(loc,1)

    return allData

def countVariants(allData):
    # this counts each type of variant
    from collections import defaultdict
    counts = defaultdict(int)

def mutationRatios(totalVarCounts):
    # this computes the ratios of each substitution changes
    total = 0
    for i in totalVarCounts:
        total += totalVarCounts[i]


    for i in totalVarCounts:
        print('%s:%s' % (i,str(float(totalVarCounts[i])/float(total))))

def defineProbabilities():
    #x = {'T-A':0.2886282509,'T-G':0.008669046405,'T-C':0.7027027027,'C-A':0.1144487215,'C-G':0.005361561727,'C-T':0.8801897168,'G-C':0.1411374931,'G-A':0.3717283269,'G-T':0.48713418,'A-G':0.6341116599,'A-C':0.006061966771,'A-T':0.3598263733}
    x = {'T-A':0.1045736252,'T-G':0.004353965639,'T-C':0.2457048717,'T-T':0.6453675375,'C-A':0.07028799544,'C-G':0.003813800969,'C-T':0.506201882,'C-C':0.4196963216,'G-C':0.05708131395,'G-A':0.1481652435,'G-T':0.1937841372,'G-G':0.6009693053,'A-G':0.3658376711,'A-C':0.004704067862,'A-T':0.2100250627,'A-A':0.4194331984}

    return x

def calculateProbs(allData, ratios):
    from collections import defaultdict
    from scipy.misc import comb
    allProbs = defaultdict(list)

    for loc in allData:
        count = 0
        varType = ''
        for variant in allData[loc]:
            # count variant instances
            if pd.notnull(variant) and len(variant) == 3:
                varType = variant
                count += 1
        if count > 0:
            # simple probability
            #probability = pow(ratios[varType],count*2)

            # binomial probability
            probability = comb(21,count)*pow(ratios[varType],count)*pow((1-ratios[varType]),(21-count))
            allProbs[varType].append(probability)


    # now sort all of the lists in descending order
    for i in allProbs:
        list.sort(allProbs[i],reverse=True)

    # calculate p values
    fisherpvalues(allProbs)

    return allProbs

def fisherpvalues(allProbs):
    # calculate p value by fisher's test
    # will return a dictionary with p values for each variant
    from scipy.stats import combine_pvalues as fisher

    for i in allProbs:
        stats = fisher(allProbs[i])
        chi = stats[0]
        p_val = stats[1]
        print i, p_val




def plotProbabilities(allProbs):
    import matplotlib.pyplot as plt
    # plot as p values
    for i in allProbs:
        plt.bar(range(len(allProbs[i])),allProbs[i],color='gray')
        plt.title('%s' % (i))
        plt.ylabel('p-value')

        plt.show()


            




    

    


if __name__ == '__main__':
    import pandas as pd
    from collections import defaultdict
    inDir = runArgparse()
    fileList = getFiles(inDir)
    allData = pd.DataFrame()
    badLocs = [] # keep track of locs with multiple vars in single vcf
    goodLocs = {} # keep track of variant type at each loc
    totalVarCounts = defaultdict(int)
    # parse all input files
    for inputFile in fileList:
        df, badLocs, totalVarCounts = buildDF(inputFile, badLocs, goodLocs, totalVarCounts)
        allData = combineDFs(allData, df)
    # remove any missed badlocs
    allData = removeBadLocs(allData, badLocs)
    countVariants(allData)
    #mutationRatios(totalVarCounts)
    ratios = defineProbabilities()
    # calculate all the probabilites for every locus
    allProbs = calculateProbs(allData, ratios)
    plotProbabilities(allProbs)





